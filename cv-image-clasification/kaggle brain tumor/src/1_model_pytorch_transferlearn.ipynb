{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Brain Tumor Video Classification using PyTorch + W&B Tracking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import platform\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "import einops\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import timm\n",
    "import glob\n",
    "import cv2\n",
    "import pydicom as dicom\n",
    "\n",
    "from rich import print as _pprint\n",
    "from rich.progress import track\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "working_dir = '/home/denis/AI/python/clasificador de imagen/kaggle brain tumor/data'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "#Código para saber cuantas imagenes tiene cada ejemplo (minimo y maximo)\n",
    "working_dir2 = working_dir + '/rsna-miccai-brain-tumor-radiogenomic-classification' + '/train'\n",
    "samples = os.listdir(working_dir2)\n",
    "len_list = []\n",
    "for s in samples:\n",
    "    working_dir3 = working_dir2 + '/' + s + '/FLAIR'\n",
    "    len_list.append(len(os.listdir(working_dir3)))\n",
    "\n",
    "print(np.array(len_list).min(), np.array(len_list).max())\n",
    "\n",
    "\n",
    "r = np.random.rand(2,5,3)\n",
    "z = np.zeros((4,5,3))\n",
    "s = np.concatenate((r,z), axis=0)\n",
    "s.shape\n",
    "\n",
    "\n",
    "frames_tr\n",
    "h, w = frames_tr.shape \n",
    "\n",
    "z = np.zeros( (h,w,,max(100)-))\n",
    "MAX_FRAMES"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15 514\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6, 5, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def get_patient_id(patient_id):\n",
    "    if patient_id < 10:\n",
    "        return '0000'+str(patient_id)\n",
    "    elif patient_id >= 10 and patient_id < 100:\n",
    "        return '000'+str(patient_id)\n",
    "    elif patient_id >= 100 and patient_id < 1000:\n",
    "        return '00'+str(patient_id)\n",
    "    else:\n",
    "        return '0'+str(patient_id)\n",
    "\n",
    "def get_path(row):\n",
    "    patient_id = get_patient_id(row.BraTS21ID)\n",
    "    return f'{working_dir}/rsna-miccai-brain-tumor-radiogenomic-classification/train/{patient_id}/FLAIR/'\n",
    "\n",
    "def wandb_log(**kwargs):\n",
    "    \"\"\"\n",
    "    Logs a key-value pair to W&B\n",
    "    \"\"\"\n",
    "    for k, v in kwargs.items():\n",
    "        wandb.log({k: v})\n",
    "        \n",
    "def cprint(string):\n",
    "    \"\"\"\n",
    "    Utility function for beautiful colored printing.\n",
    "    \"\"\"\n",
    "    _pprint(f\"[black]{string}[/black]\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config Dictionary and W&B Integration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "Config = dict(\n",
    "    MAX_FRAMES = 300,\n",
    "    EPOCHS = 5,\n",
    "    LR = 2e-4,\n",
    "    IMG_SIZE = (224, 224),\n",
    "    FEATURE_EXTRACTOR = 'resnext50_32x4d',\n",
    "    DR_RATE = 0.35,\n",
    "    NUM_CLASSES = 1,\n",
    "    RNN_HIDDEN_SIZE = 100,\n",
    "    RNN_LAYERS = 1,\n",
    "    TRAIN_BS = 4,\n",
    "    VALID_BS = 4,\n",
    "    NUM_WORKERS = 4,\n",
    "    infra = \"Kaggle\",\n",
    "    competition = 'rsna_miccai',\n",
    "    _wandb_kernel = 'tanaym'\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    project='kaggle_brain_tumor',\n",
    "    config=Config,\n",
    "    group='vision',\n",
    "    job_type='train',\n",
    "    anonymous='allow'\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Finishing last run (ID:1m2mbjhq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26274<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5f099c06456457d92478cbf2f64cfc8"
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/denis/AI/python/configuraciones/wandb/run-20210917_225447-1m2mbjhq/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/denis/AI/python/configuraciones/wandb/run-20210917_225447-1m2mbjhq/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>batch_train_loss</td><td>0.56779</td></tr><tr><td>batch_val_loss</td><td>0.68992</td></tr><tr><td>train_loss</td><td>0.69073</td></tr><tr><td>valid_loss</td><td>0.68715</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>batch_train_loss</td><td>▇▅▇▃▅▄█▃▃▄▃▂▄▁▇▅▃▆▂▅▄▄▄▃▅▁▄▃▃▃▁▄▄▄▆▃▁▄▂▃</td></tr><tr><td>batch_val_loss</td><td>▃▁▄▇█▄▄▃▅▆█▆▆▆▅▆▆██▇▇▇▆▆▆▇▅▇█▅▆▇▆▇▆▇▇▆▆▆</td></tr><tr><td>train_loss</td><td>▇█▁▂▂</td></tr><tr><td>valid_loss</td><td>▃▁▇█▂</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">wandering-forest-11</strong>: <a href=\"https://wandb.ai/denisquant/kaggle_brain_tumor/runs/1m2mbjhq\" target=\"_blank\">https://wandb.ai/denisquant/kaggle_brain_tumor/runs/1m2mbjhq</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:1m2mbjhq). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">devoted-flower-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/denisquant/kaggle_brain_tumor\" target=\"_blank\">https://wandb.ai/denisquant/kaggle_brain_tumor</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/denisquant/kaggle_brain_tumor/runs/3f0mztar\" target=\"_blank\">https://wandb.ai/denisquant/kaggle_brain_tumor/runs/3f0mztar</a><br/>\n",
       "                Run data is saved locally in <code>/home/denis/AI/python/configuraciones/wandb/run-20210917_232623-3f0mztar</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "class Augments:\n",
    "    \"\"\"\n",
    "    Contains Train, Validation Augments\n",
    "    \"\"\"\n",
    "    train_augments = A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ],p=1.)\n",
    "    \n",
    "    valid_augments = A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], p=1.)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Dataset Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this custom Dataset, I am essentially reading \"MAX_FRAMES\" number of images from a patient's FLAIR folder and making list of those frames and converting it to torch tensor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "class RSNADataset(Dataset):\n",
    "    def __init__(self, df, augments=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.augments = augments\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        row = self.df.loc[idx]\n",
    "        paths = self.getPaths(row)\n",
    "        frames = []\n",
    "        for path in paths:\n",
    "            data_file = dicom.dcmread(path)\n",
    "            img = data_file.pixel_array\n",
    "            img = img.astype(np.uint8)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            img = cv2.resize(img, Config['IMG_SIZE'])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            frames.append(img)\n",
    "\n",
    "        frames_tr = np.stack(frames, axis=2)\n",
    "        \n",
    "        if self.augments:\n",
    "            for frame in frames:\n",
    "                frame = self.augments(image=frame)['image']\n",
    "                frames_tr.append(frame)\n",
    "            \n",
    "        if self.is_test:\n",
    "            return frames_tr\n",
    "        else:\n",
    "            label = torch.tensor(row['MGMT_value']).float()\n",
    "            return frames_tr, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def getPaths(self, row):\n",
    "        paths = glob.glob(row['path'] + '*.dcm')\n",
    "        sortedPaths = self.sort(paths)\n",
    "        maxWindowStart = len(sortedPaths) - Config['MAX_FRAMES']\n",
    "        start = 0 # np.random.randint(1, maxWindowStart)\n",
    "        paths = sortedPaths[start:Config['MAX_FRAMES']]\n",
    "        \n",
    "        return paths\n",
    "        \n",
    "    def sort(self, entry):\n",
    "        # https://stackoverflow.com/a/2669120/7636462\n",
    "        convert = lambda text: int(text) if text.isdigit() else text \n",
    "        alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    \n",
    "        return sorted(entry, key = alphanum_key)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Class with ResNext Backbone"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "class ResNextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNextModel, self).__init__()\n",
    "        self.backbone = timm.create_model(Config['FEATURE_EXTRACTOR'], pretrained=True, in_chans=1)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class RSNAModel(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(RSNAModel, self).__init__()\n",
    "        self.backbone = ResNextModel()\n",
    "        num_features = self.backbone.backbone.fc.in_features\n",
    "        \n",
    "        self.backbone.backbone.fc = Identity()\n",
    "        self.dropout= nn.Dropout(Config['DR_RATE'])\n",
    "        self.rnn = nn.LSTM(num_features, Config['RNN_HIDDEN_SIZE'], Config['RNN_LAYERS'])\n",
    "        self.fc1 = nn.Linear(Config['RNN_HIDDEN_SIZE'], Config['NUM_CLASSES'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b_z, fr, h, w = x.shape\n",
    "        ii = 0\n",
    "        in_pass = x[:, ii].unsqueeze_(1)\n",
    "        y = self.backbone((in_pass))\n",
    "        output, (hn, cn) = self.rnn(y.unsqueeze(1))\n",
    "        for ii in range(1, fr):\n",
    "            y = self.backbone((x[:, ii].unsqueeze_(1)))\n",
    "            out, (hn, cn) = self.rnn(y.unsqueeze(1), (hn, cn))\n",
    "        out = self.dropout(out[:, -1])\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Validation Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "def train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch, device, log_wandb=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Trains model for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    prog_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    \n",
    "    for batch, (frames, targets) in prog_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        frames = frames.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.float)\n",
    "        \n",
    "        # Re arrange the frames in the format our model wants to recieve\n",
    "        frames = einops.rearrange(frames, 'b h w f -> b f h w')\n",
    "\n",
    "        preds = model(frames).view(-1)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_item = loss.item()\n",
    "        running_loss += loss_item\n",
    "        \n",
    "        prog_bar.set_description(f\"loss: {loss_item:.4f}\")\n",
    "        \n",
    "        if log_wandb == True:\n",
    "            wandb_log(\n",
    "                batch_train_loss=loss_item\n",
    "            )\n",
    "        \n",
    "        if verbose == True and batch % 20 == 0:\n",
    "            print(f\"Batch: {batch}, Loss: {loss_item}\")\n",
    "    \n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, valid_dataloader, loss_fn, epoch, device, log_wandb=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    prog_bar = tqdm(enumerate(valid_dataloader), total=len(valid_dataloader))\n",
    "    for batch, (frames, targets) in prog_bar:\n",
    "        frames = frames.to(device, torch.float)\n",
    "        targets = targets.to(device, torch.float)\n",
    "        \n",
    "        # Re arrange the frames in the format our model wants to recieve\n",
    "        frames = einops.rearrange(frames, 'b h w f -> b f h w')\n",
    "        preds = model(frames).view(-1)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        \n",
    "        loss_item = loss.item()\n",
    "        running_loss += loss_item\n",
    "        \n",
    "        prog_bar.set_description(f\"val_loss: {loss_item:.4f}\")\n",
    "        \n",
    "        if log_wandb == True:\n",
    "            wandb_log(\n",
    "                batch_val_loss=loss_item\n",
    "            )\n",
    "        \n",
    "        if verbose == True and batch % 10 == 0:\n",
    "            print(f\"Batch: {batch}, Loss: {loss_item}\")\n",
    "    \n",
    "    avg_val_loss = running_loss / len(valid_dataloader)\n",
    "    \n",
    "    return avg_val_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    log_wandb = True\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        print(\"\\nGPU not found. Using CPU: {}\\n\".format(platform.processor()))\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    \n",
    "    # Load training csv file\n",
    "    df = pd.read_csv(working_dir+'/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv')\n",
    "    df['path'] = df.apply(lambda row: get_path(row), axis=1)\n",
    "\n",
    "    # Removing two patient ids from the dataframe since there are not FLAIR directories for these ids. \n",
    "    df = df.loc[df.BraTS21ID!=109]\n",
    "    df = df.loc[df.BraTS21ID!=709]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    train_df, valid_df = train_test_split(df, test_size=0.1, stratify=df.MGMT_value.values)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    valid_df = valid_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f'Size of Training Set: {len(train_df)}, Validation Set: {len(valid_df)}')\n",
    "    \n",
    "    model = RSNAModel()\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config['LR'])\n",
    "\n",
    "    train_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    valid_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    print(f\"\\nUsing Backbone: {Config['FEATURE_EXTRACTOR']}\")\n",
    "    \n",
    "    train_data = RSNADataset(train_df)\n",
    "    valid_data = RSNADataset(valid_df)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=Config['TRAIN_BS'], \n",
    "        shuffle=True,\n",
    "        num_workers=Config['NUM_WORKERS']\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_data, \n",
    "        batch_size=Config['VALID_BS'], \n",
    "        shuffle=False,\n",
    "        num_workers=Config['NUM_WORKERS']\n",
    "    )\n",
    "    \n",
    "    current_loss = 1000\n",
    "    for epoch in range(Config['EPOCHS']):\n",
    "        print(f\"\\n{'--'*8} EPOCH: {epoch+1} {'--'*8}\\n\")\n",
    "        \n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, train_loss_fn, epoch=epoch, device=device, log_wandb=log_wandb)\n",
    "        \n",
    "        valid_loss = valid_one_epoch(model, valid_loader, valid_loss_fn, epoch=epoch, device=device, log_wandb=log_wandb)\n",
    "        \n",
    "        print(f\"val_loss: {valid_loss:.4f}\")\n",
    "        \n",
    "        if log_wandb == True:\n",
    "            wandb_log(\n",
    "                train_loss=train_loss,\n",
    "                valid_loss=valid_loss\n",
    "            )\n",
    "        \n",
    "        if valid_loss < current_loss:\n",
    "            current_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"model_{Config['FEATURE_EXTRACTOR']}.pt\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060\n",
      "\n",
      "Size of Training Set: 524, Validation Set: 59\n",
      "\n",
      "Using Backbone: resnext50_32x4d\n",
      "\n",
      "---------------- EPOCH: 1 ----------------\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/131 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 56, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [224, 224, 23] at entry 0 and [224, 224, 129] at entry 1\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24785/1665942485.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{'--'*8} EPOCH: {epoch+1} {'--'*8}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_wandb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_wandb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_wandb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_wandb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24785/3632033925.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_dataloader, optimizer, loss_fn, epoch, device, log_wandb, verbose)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprog_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepsing/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/denis/miniconda3/envs/deepsing/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 56, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [224, 224, 23] at entry 0 and [224, 224, 129] at entry 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('deepsing': conda)"
  },
  "interpreter": {
   "hash": "3286054ad7c5d2dac794c31c55bee32c349aa02b1aa3c06f9a8f320385bbb867"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}